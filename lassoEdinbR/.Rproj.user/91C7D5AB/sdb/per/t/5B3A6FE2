{
    "collab_server" : "",
    "contents" : "How to lasso: Least Absolute Shrinkage and Selection Operator\n========================================================\nauthor: Drew Altschul, Department of Psychology\ndate: \nautosize: true\ntransition: none\n\n\n  \nThe Problem of Model Building\n========================================================\n\n\nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  \n  \nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  - Inherently subjective stepwise procedures\n  \n  \nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  - Inherently subjective stepwise procedures\n  - which aren't even good!\n\n\nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  - Inherently subjective stepwise procedures\n  - which aren't even good!\n- Can modern computing power improve model selection procedures?\n\n\nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  - Inherently subjective stepwise procedures\n  - which aren't even good!\n- Can modern computing power improve model selection procedures?\n- The lasso\n\n\nThe Problem of Model Building\n========================================================\n\n- How can we eliminate the subjectivity of model selection?\n  - Inherently subjective stepwise procedures\n  - which aren't even good!\n- Can modern computing power improve model selection procedures?\n- The lasso\n  - shrinking variable estimates to zero\n\n\nWhat is the lasso?\n========================================================\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n  - reduce complexity\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n  - reduce complexity\n  - increase generalizability\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n  - reduce complexity\n  - increase generalizability\n- Similar to ridge regression\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n  - reduce complexity\n  - increase generalizability\n- Similar to ridge regression\n  - but lasso can shrink to zero\n\n\nWhat is the lasso?\n========================================================\n\n- Regularization method\n- Penalizes estimates \n  - reduce complexity\n  - increase generalizability\n- Similar to ridge regression\n  - but lasso can shrink to zero\n  - and usually fits better\n\n  \nWhat does the lasso do?\n========================================================\n\nBasic linear model: $\\hat{y} = b_0 + b_1*x_1+ b_2*x_2 + ... b_p*x_p$\n\n\nWhat does the lasso do?\n========================================================\n\nBasic linear model: $\\hat{y} = b_0 + b_1*x_1+ b_2*x_2 + ... b_p*x_p$\n\nThe lasso fits this with criteria\n* minimize: $\\sum (y - \\hat{y})^2$\n* such that: $\\sum |b_j| \\leq s$\n\n\nWhat does the lasso do?\n========================================================\n\nBasic linear model: $\\hat{y} = b_0 + b_1*x_1+ b_2*x_2 + ... b_p*x_p$\n\nThe lasso fits this with criteria\n* minimize: $\\sum (y - \\hat{y})^2$\n* such that: $\\sum |b_j| \\leq s$\n\n- when $s$ is large, the constraint has no effect and the solution is the usual multiple regression\n- and $s$ becomes small, the coefficients are shrunk, sometimes even to 0\n\n\nFeatures of the lasso\n========================================================\n\n  \nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  \n\nFeatures of the lasso\n========================================================\n![optional caption text](biasEst.png)\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n  - Does not produce standard errors\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n  - Does not produce standard errors\n- Preforms well when data are sparse\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n  - Does not produce standard errors\n- Preforms well when data are sparse\n- Copes with correlated variables\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n  - Does not produce standard errors\n- Preforms well when data are sparse\n- Copes with correlated variables\n- Can fit data with more variables than observations\n\n\nFeatures of the lasso\n========================================================\n- Estimates are biased due to penalization\n  - Bias-variance trade-off\n  - lasso estimates usually more accurate, if biased\n  - Does not produce standard errors\n- Preforms well when data are sparse\n- Copes with correlated variables\n- Can fit data with more variables than observations\n  - particularly good at this is a variant called the Elastic Net\n\n\nWorking with the elastic net\n========================================================\n\n\nWorking with the elastic net\n========================================================\nElastic nets use a mixing parameter $\\alpha$ to combine lasso and ridge regression\n\n\nWorking with the elastic net\n========================================================\nElastic nets use a mixing parameter $\\alpha$ to combine lasso and ridge regression\n\n1. Parameter optimization\n\n\nWorking with the elastic net\n========================================================\nElastic nets use a mixing parameter $\\alpha$ to combine lasso and ridge regression\n\n1. Parameter optimization\n2. Prediction\n\n\nWorking with the elastic net\n========================================================\nElastic nets use a mixing parameter $\\alpha$ to combine lasso and ridge regression\n\n1. Parameter optimization\n2. Prediction\n\nPackage `glmnet`\n\n\nWorking with the elastic net\n========================================================\nElastic nets use a mixing parameter $\\alpha$ to combine lasso and ridge regression\n\n1. Parameter optimization\n2. Prediction\n\nPackage `glmnet`\n\nglmnet works with binomial, multinomial, poisson, & cox models\n\n\nRelated Packages\n========================================================\n* Cross Validation\n  - `tune {e1071}`\n* Mixed Models\n  - `glmmLasso`\n* Genomics\n  - `LDlasso`\n* Bayesian\n  - `EBglmnet`\n* Hazard models\n  - `ahaz`\n  \n***\n\n* Significance testing\n  - `covTest`\n* SEM\n  - `regSEM`\n  - `sparseSEM`\n  - `qgraph`\n* Variable, or rather, stability selection\n  - `stabs`\n  - `c060`\n  \n\nStability selection\n========================================================\n\n\n\nStability selection\n========================================================\nIf you're using glmnet to its fullest potential, \n  in many cases you won't need variable selection anymore\n\n\nStability selection\n========================================================\nIf you're using glmnet to its fullest potential, \n  in many cases you won't need variable selection anymore\n\nBut if you do\n\n\nStability selection\n========================================================\nIf you're using glmnet to its fullest potential, \n  in many cases you won't need variable selection anymore\n\nBut if you do\n\nStability selection will allow you to use these regularization techniques to identify which variables consistently contribute to the model\n\n\nSummary\n========================================================\n\n\nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  \n\nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  * for both prediction modeling\n  \n  \nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  * for both prediction modeling\n  * and variable selection\n\n\nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  * for both prediction modeling\n  * and variable selection\n* highly generalizable   \n\n\nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  * for both prediction modeling\n  * and variable selection\n* highly generalizable   \n* good support in R\n\n\nSummary\n========================================================\n* the lasso is a flexible, effective tool\n  * for both prediction modeling\n  * and variable selection\n* highly generalizable   \n* good support in R\n* not hard to get the hang of \n\n\nQuestions\n========================================================\n\n* dremalt@gmail.com\n\n* @dremalt\n",
    "created" : 1473938772690.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1734879238",
    "id" : "5B3A6FE2",
    "lastKnownWriteTime" : 1479727591,
    "last_content_update" : 1479727591729,
    "path" : "C:/Users/s1229179/GitHub/R/lassoEdinbR/howtolasso.Rpres",
    "project_path" : "howtolasso.Rpres",
    "properties" : {
        "docOutlineVisible" : "0",
        "ignored_words" : "glmnet,generalizable,dremalt,stepwise,generalizability,leq\n"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_presentation"
}