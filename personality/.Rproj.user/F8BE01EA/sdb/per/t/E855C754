{
    "contents" : "################################################################################################################\n# To use the program code in this .R file, it should be imported into R using the \"Source R code\" option from\n# the \"File\" menu in R.  \n#\n# To run the main program (EFA.Comp.Data), you need to provide data (arranged as an N [sample size] x k [number \n# of variables] matrix) and specify the largest number of factors to consider.  The program may reach a stopping\n# point before it reaches this largest number of factors.  The Sample.Commands program provides two\n# illustrations for how to use the EFA.Comp.Data program.\n\n\n################################################################################################################\nSample.Commands <- function()\n{\n  # This program shows how to use the main program with two illustrative samples of data.  The first sample \n  # contains N = 500 cases, k = 12 normally distributed continuous variables, and 3 correlated factors.  The \n  # sample commands show the correlation matrix and the output of the EFA.Comp.Data program, which correctly \n  # identifies 3 factors.\n  #\n  # The second sample contains N = 500 cases, k = 8 non-normally distributed ordinal variables, and 2 \n  # uncorrelated factors.  The sample commands show the Pearson correlation matrix, the Spearman correlation \n  # matrix, the difference between these matrices, the output of the EFA.Comp.Data program (first using\n  # Pearson correlations and then using Spearman correlations).  The first run overidentifies the number of\n  # factors, but the second correctly identifies 2 factors.\n  \n  set.seed(1)\n  s1 <- rnorm(500)\n  s2 <- rnorm(500)\n  s3 <- rnorm(500)\n  x1 <- s1 + s2 + rnorm(500)\n  x2 <- s1 + s2 + rnorm(500)\n  x3 <- s1 + s2 + rnorm(500)\n  x4 <- s1 + s2 + rnorm(500)\n  x5 <- s1 + s3 + rnorm(500)\n  x6 <- s1 + s3 + rnorm(500)\n  x7 <- s1 + s3 + rnorm(500)\n  x8 <- s1 + s3 + rnorm(500)\n  x9 <- s2 + s3 + rnorm(500)\n  x10 <- s2 + s3 + rnorm(500)\n  x11 <- s2 + s3 + rnorm(500)\n  x12 <- s2 + s3 + rnorm(500)\n  x <- cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12)\n  print(round(cor(x), 2))\n  EFA.Comp.Data(Data = x, F.Max = 5, Graph = T)\n  \n  s1 <- runif(500)\n  s2 <- runif(500)\n  x1 <- cut(((s1 + .5 * runif(500)) / 1.5) ^ .5, breaks = 5)\n  x2 <- cut(((s1 + .5 * runif(500)) / 1.5) ^ .5, breaks = 5)\n  x3 <- cut(((s1 + .5 * runif(500)) / 1.5) ^ 2, breaks = 5)\n  x4 <- cut(((s1 + .5 * runif(500)) / 1.5) ^ 2, breaks = 5)\n  x5 <- cut(((s2 + .5 * runif(500)) / 1.5) ^ .5, breaks = 5)\n  x6 <- cut(((s2 + .5 * runif(500)) / 1.5) ^ .5, breaks = 5)\n  x7 <- cut(((s2 + .5 * runif(500)) / 1.5) ^ 2, breaks = 5)\n  x8 <- cut(((s2 + .5 * runif(500)) / 1.5) ^ 2, breaks = 5)\n  x <- cbind(x1,x2,x3,x4,x5,x6,x7,x8)\n  print(round(cor(x), 2))\n  print(round(cor(x), 2), method = \"spearman\")\n  print(round(cor(x) - cor(x, method = \"spearman\"), 2))\n  win.graph()\n  EFA.Comp.Data(Data = x, F.Max = 5, Graph = T)\n  win.graph()\n  EFA.Comp.Data(Data = x, F.Max = 5, Graph = T, Spearman = T)\n}\n\n\n################################################################################################################\nEFA.Comp.Data <- function(Data, F.Max, N.Pop = 10000, N.Samples = 500, Alpha = .05, Graph = F, Spearman = F)\n{\n  # Data = N (sample size) x k (number of variables) data matrix\n  # F.Max = largest number of factors to consider\n  # N.Pop = size of finite populations of comparison data (default = 10,000 cases)\n  # N.Samples = number of samples drawn from each population (default = 500)\n  # Alpha = alpha level when testing statistical significance of improvement with add'l factor (default = .30) \n  # Graph = whether to plot the fit of eigenvalues to those for comparison data (default = F)\n  # Spearman = whether to use Spearman rank-order correlations rather than Pearson correlations (default = F)\n  \n  N <- dim(Data)[1]\n  k <- dim(Data)[2]\n  if (Spearman) Cor.Type <- \"spearman\" else Cor.Type <- \"pearson\"\n  cor.Data <- cor(Data, method = Cor.Type)\n  Eigs.Data <- eigen(cor.Data)$values\n  RMSR.Eigs <- matrix(0, nrow = N.Samples, ncol = F.Max)\n  Sig <- T\n  F.CD <- 1\n  while ((F.CD <= F.Max) & (Sig))\n  {\n    Pop <- GenData(Data, N.Factors = F.CD, N = N.Pop, Cor.Type = Cor.Type)\n    for (j in 1:N.Samples)\n    {\n      Samp <- Pop[sample(1:N.Pop, size = N, replace = T),]\n      cor.Samp <- cor(Samp, method = Cor.Type)\n      Eigs.Samp <- eigen(cor.Samp)$values\n      RMSR.Eigs[j,F.CD] <- sqrt(sum((Eigs.Samp - Eigs.Data) * (Eigs.Samp - Eigs.Data)) / k)\n    }\n    if (F.CD > 1) Sig <- (wilcox.test(RMSR.Eigs[,F.CD], RMSR.Eigs[,(F.CD - 1)], \"less\")$p.value < Alpha)\n    if (Sig) F.CD <- F.CD + 1\n  }\n  cat(\"Number of factors to retain: \",F.CD - 1,\"\\n\")\n  if (Graph)\n  {\n    if (Sig) x.max <- F.CD - 1\n    else x.max <- F.CD\n    ys <- apply(RMSR.Eigs[,1:x.max], 2, mean)\n    plot(x = 1:x.max, y = ys, ylim = c(0, max(ys)), xlab = \"Factor\", ylab = \"RMSR Eigenvalue\", type = \"b\", \n         main = \"Fit to Comparison Data\")\n    abline(v = F.CD - 1, lty = 3)\n  }\n}\n\n\n################################################################################################################\nGenData <- function(Supplied.Data, N.Factors, N, Max.Trials = 5, Initial.Multiplier = 1, Cor.Type)\n{\n  # Steps refer to description in the following article:\n  # Ruscio, J., & Kaczetow, W. (2008). Simulating multivariate nonnormal data using an iterative algorithm. \n  # Multivariate Behavioral Research, 43(3), 355-381.\n  \n  # Initialize variables and (if applicable) set random number seed (step 1) -------------------------------------\n  \n  set.seed(1)\n  k <- dim(Supplied.Data)[2]\n  Data <- matrix(0, nrow = N, ncol = k)            # Matrix to store the simulated data\n  Distributions <- matrix(0, nrow = N, ncol = k)   # Matrix to store each variable's score distribution\n  Iteration <- 0                                   # Iteration counter\n  Best.RMSR <- 1                                   # Lowest RMSR correlation\n  Trials.Without.Improvement <- 0                  # Trial counter\n  \n  # Generate distribution for each variable (step 2) -------------------------------------------------------------\n  \n  for (i in 1:k)\n    Distributions[,i] <- sort(sample(Supplied.Data[,i], size = N, replace = T))\n  \n  # Calculate and store a copy of the target correlation matrix (step 3) -----------------------------------------\n  \n  Target.Corr <- cor(Supplied.Data, method = Cor.Type)\n  Intermediate.Corr <- Target.Corr\n  \n  # Generate random normal data for shared and unique components, initialize factor loadings (steps 5, 6) --------\n  \n  Shared.Comp <- matrix(rnorm(N * N.Factors, 0, 1), nrow = N, ncol = N.Factors)\n  Unique.Comp <- matrix(rnorm(N * k, 0, 1), nrow = N, ncol = k)\n  Shared.Load <- matrix(0, nrow = k, ncol = N.Factors)\n  Unique.Load <- matrix(0, nrow = k, ncol = 1)\n  \n  # Begin loop that ends when specified number of iterations pass without improvement in RMSR correlation --------\n  \n  while (Trials.Without.Improvement < Max.Trials)\n  {\n    Iteration <- Iteration + 1\n    \n    # Calculate factor loadings and apply to reproduce desired correlations (steps 7, 8) ---------------------------\n    \n    Fact.Anal <- Factor.Analysis(Intermediate.Corr, Corr.Matrix = TRUE, N.Factors = N.Factors, Cor.Type = Cor.Type)\n    if (N.Factors == 1) Shared.Load[,1] <- Fact.Anal$loadings\n    else \n      for (i in 1:N.Factors)\n        Shared.Load[,i] <- Fact.Anal$loadings[,i]\n    Shared.Load[Shared.Load > 1] <- 1\n    Shared.Load[Shared.Load < -1] <- -1\n    if (Shared.Load[1,1] < 0) Shared.Load <- Shared.Load * -1\n    for (i in 1:k)\n      if (sum(Shared.Load[i,] * Shared.Load[i,]) < 1) Unique.Load[i,1] <- \n      (1 - sum(Shared.Load[i,] * Shared.Load[i,]))\n    else Unique.Load[i,1] <- 0\n    Unique.Load <- sqrt(Unique.Load)\n    for (i in 1:k)\n      Data[,i] <- (Shared.Comp %*% t(Shared.Load))[,i] + Unique.Comp[,i] * Unique.Load[i,1]\n    \n    # Replace normal with nonnormal distributions (step 9) ---------------------------------------------------------\n    \n    for (i in 1:k)\n    {\n      Data <- Data[sort.list(Data[,i]),]\n      Data[,i] <- Distributions[,i]\n    }\n    \n    # Calculate RMSR correlation, compare to lowest value, take appropriate action (steps 10, 11, 12) --------------\n    \n    Reproduced.Corr <- cor(Data, method = Cor.Type)\n    Residual.Corr <- Target.Corr - Reproduced.Corr\n    RMSR <- sqrt(sum(Residual.Corr[lower.tri(Residual.Corr)] * Residual.Corr[lower.tri(Residual.Corr)]) / \n                   (.5 * (k * k - k)))\n    if (RMSR < Best.RMSR)\n    {\n      Best.RMSR <- RMSR\n      Best.Corr <- Intermediate.Corr\n      Best.Res <- Residual.Corr\n      Intermediate.Corr <- Intermediate.Corr + Initial.Multiplier * Residual.Corr\n      Trials.Without.Improvement <- 0\n    }\n    else \n    {\n      Trials.Without.Improvement <- Trials.Without.Improvement + 1\n      Current.Multiplier <- Initial.Multiplier * .5 ^ Trials.Without.Improvement\n      Intermediate.Corr <- Best.Corr + Current.Multiplier * Best.Res\n    }\n  }\n  \n  # Construct the data set with the lowest RMSR correlation (step 13) --------------------------------------------\n  \n  Fact.Anal <- Factor.Analysis(Best.Corr, Corr.Matrix = TRUE, N.Factors = N.Factors, Cor.Type = Cor.Type)\n  if (N.Factors == 1) Shared.Load[,1] <- Fact.Anal$loadings\n  else\n    for (i in 1:N.Factors)\n      Shared.Load[,i] <- Fact.Anal$loadings[,i]\n  Shared.Load[Shared.Load > 1] <- 1\n  Shared.Load[Shared.Load < -1] <- -1\n  if (Shared.Load[1,1] < 0) Shared.Load <- Shared.Load * -1\n  for (i in 1:k)\n    if (sum(Shared.Load[i,] * Shared.Load[i,]) < 1) Unique.Load[i,1] <-\n    (1 - sum(Shared.Load[i,] * Shared.Load[i,]))\n  else Unique.Load[i,1] <- 0\n  Unique.Load <- sqrt(Unique.Load)\n  for (i in 1:k)\n    Data[,i] <- (Shared.Comp %*% t(Shared.Load))[,i] + Unique.Comp[,i] * Unique.Load[i,1]\n  Data <- apply(Data, 2, scale) # standardizes each variable in the matrix\n  for (i in 1:k)\n  {\n    Data <- Data[sort.list(Data[,i]),]\n    Data[,i] <- Distributions[,i]\n  }\n  \n  # Return the simulated data set (step 14) ----------------------------------------------------------------------\n  \n  return(Data)\n}\n\n################################################################################################################\nFactor.Analysis <- function(Data, Corr.Matrix = FALSE, Max.Iter = 50, N.Factors = 0, Cor.Type)\n{\n  Data <- as.matrix(Data)\n  k <- dim(Data)[2]\n  if (N.Factors == 0)\n  {\n    N.Factors <- k\n    Determine <- T\n  }\n  else Determine <- F\n  if (!Corr.Matrix) Cor.Matrix <- cor(Data, method = Cor.Type)\n  else Cor.Matrix <- Data\n  Criterion <- .001\n  Old.H2 <- rep(99, k)\n  H2 <- rep(0, k)\n  Change <- 1\n  Iter <- 0\n  Factor.Loadings <- matrix(nrow = k, ncol = N.Factors)\n  while ((Change >= Criterion) & (Iter < Max.Iter))\n  {\n    Iter <- Iter + 1\n    Eig <- eigen(Cor.Matrix)\n    L <- sqrt(Eig$values[1:N.Factors])\n    for (i in 1:N.Factors)\n      Factor.Loadings[,i] <- Eig$vectors[,i] * L[i]\n    for (i in 1:k)\n      H2[i] <- sum(Factor.Loadings[i,] * Factor.Loadings[i,])\n    Change <- max(abs(Old.H2 - H2))\n    Old.H2 <- H2\n    diag(Cor.Matrix) <- H2\n  }\n  if (Determine) N.Factors <- sum(Eig$values > 1)\n  return(list(loadings = Factor.Loadings[,1:N.Factors], factors = N.Factors))\n}\n",
    "created" : 1374096873876.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2434612921",
    "id" : "E855C754",
    "lastKnownWriteTime" : 1374097678,
    "path" : "C:/Users/Diapadion/Dropbox/R/personality/CD-EFA.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}